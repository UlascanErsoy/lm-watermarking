{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee65e30-6775-41dc-b3d2-1f11744c8799",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Watermarking Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58023e2-e009-4337-b92d-41b1e2e648e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available! NVIDIA GeForce RTX 3070 Ti Laptop GPU || VRAM:  7 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        print(f\"Cuda is available! {torch.cuda.get_device_name()} || VRAM: {torch.cuda.mem_get_info()[0] // 1e9: .0f} GB\")\n",
    "else:\n",
    "        print(\"Cuda is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1edd15ac-7dd9-45c2-abfc-ee10b5c3f2cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "import numpy # for gradio hot reload\n",
    "\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForCausalLM,\n",
    "                          LogitsProcessorList)\n",
    "\n",
    "from extended_watermark_processor import WatermarkLogitsProcessor, WatermarkDetector, EntropiesLogitsProcessor\n",
    "from sweet import SweetDetector\n",
    "from html_css_base import CSS_BASE, SPINNER\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b9cb1-eb3c-4b5c-afbc-2e365fb1ffcc",
   "metadata": {},
   "source": [
    "## Watermark Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ef816e-7531-4c97-ab64-779c66b24ab5",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba3bde94df94664a5a62294bb901005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='microsoft/Phi-3-mini-4k-instruct', description='Model Name:', placeholder='Enter something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0044eec5991844daad135315229a4de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=200, continuous_update=False, description='Max New Tokens:', max=8096)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f641320779f149d4987f20a7b991cbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=123, continuous_update=False, description='Seed:', max=100000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddb9cde388c431c8314a9abd458e581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=2.0, continuous_update=False, description='Delta:', max=4.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcc0cfb1f0b49b9aae7e2ed63368b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.25, continuous_update=False, description='Gamma:', max=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9800f1e9a9c4aa8b3f9d1c80c2962be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=4.0, continuous_update=False, description='Z:', max=10.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c1ef44a5da449bbdca262e50855c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='simple_1', description='Seeding Scheme:', placeholder='Enter something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb911c507c104bc98be6d2f479204b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.7, continuous_update=False, description='Temp:', max=2.0, readout_format='.1f')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = widgets.Text(\n",
    "    value='microsoft/Phi-3-mini-4k-instruct',\n",
    "    placeholder='Enter something',\n",
    "    description='Model Name:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "max_new_tokens = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=0,\n",
    "    max=8096,\n",
    "    step=1,\n",
    "    description='Max New Tokens:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "gen_seed = widgets.IntSlider(\n",
    "    value=123,\n",
    "    min=0,\n",
    "    max=1e5,\n",
    "    step=1,\n",
    "    description='Seed:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "delta = widgets.FloatSlider(\n",
    "    value=2.,\n",
    "    min=0,\n",
    "    max=4.0,\n",
    "    step=0.1,\n",
    "    description='Delta:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "gamma = widgets.FloatSlider(\n",
    "    value=.25,\n",
    "    min=0,\n",
    "    max=1.0,\n",
    "    step=0.1,\n",
    "    description='Gamma:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "z_threshold = widgets.FloatSlider(\n",
    "    value=4.,\n",
    "    min=0,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Z:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ")\n",
    "\n",
    "seeding_scheme = widgets.Text(\n",
    "    value='simple_1',\n",
    "    placeholder='Enter something',\n",
    "    description='Seeding Scheme:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "sample_temp = widgets.FloatSlider(\n",
    "    value=.7,\n",
    "    min=0,\n",
    "    max=2.,\n",
    "    step=0.1,\n",
    "    description='Temp:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    ")\n",
    "display(model_name, max_new_tokens, gen_seed, delta, gamma, z_threshold,seeding_scheme,sample_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7453cc20-241f-4dc3-9190-5ed13288f13b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b301a59e5fd49d49bb88c0712bc0c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Vocab Size: 32000\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load and return the model and tokenizer\"\"\"\n",
    "    print(f\"Loading {model_name.value}\")\n",
    "    model = model = AutoModelForCausalLM.from_pretrained(model_name.value, token=os.getenv(\"HUG_TOKEN\"))\n",
    "    \n",
    "    device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name.value, token=os.getenv(\"HUG_TOKEN\"))\n",
    "    print(f\"Tokenizer Vocab Size: {tokenizer.vocab_size}\")\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78cf8391-0db7-45db-947d-8ef70282419c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, model=None, device=None, tokenizer=None, watermark=False, system_prompt=None):\n",
    "    \"\"\"Instatiate the WatermarkLogitsProcessor according to the watermark parameters\n",
    "       and generate watermarked text by passing it to the generate method of the model\n",
    "       as a logits processor. \"\"\"\n",
    "\n",
    "    watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                                    gamma=gamma.value,\n",
    "                                                    delta=delta.value,\n",
    "                                                    seeding_scheme=seeding_scheme.value,\n",
    "                                                    select_green_tokens=True)\n",
    "    entropy_processor = EntropiesLogitsProcessor()\n",
    "    \n",
    "    gen_kwargs = dict(max_new_tokens=max_new_tokens.value)\n",
    "\n",
    "    gen_kwargs.update(dict(\n",
    "        do_sample=True, \n",
    "        top_k=0,\n",
    "        temperature=sample_temp.value\n",
    "    ))\n",
    "\n",
    "\n",
    "    generate_without_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([entropy_processor]), \n",
    "        **gen_kwargs\n",
    "    )\n",
    "    generate_with_watermark = partial(\n",
    "        model.generate,\n",
    "        logits_processor=LogitsProcessorList([entropy_processor, watermark_processor]), \n",
    "        **gen_kwargs\n",
    "    )\n",
    "\n",
    "    if hasattr(model.config,\"max_position_embedding\"):\n",
    "        prompt_max_length = model.config.max_position_embeddings-max_new_tokens.value\n",
    "    else:\n",
    "        prompt_max_length = 2048-max_new_tokens.value\n",
    "\n",
    "    if system_prompt:\n",
    "        full_prompt = f\"{system_prompt}\\n\\n{prompt}\"\n",
    "    else:\n",
    "        full_prompt = prompt\n",
    "    \n",
    "    tokd_input = tokenizer(full_prompt, return_tensors=\"pt\", add_special_tokens=True, truncation=True, max_length=prompt_max_length).to(device)\n",
    "    truncation_warning = True if tokd_input[\"input_ids\"].shape[-1] == prompt_max_length else False\n",
    "    redecoded_input = tokenizer.batch_decode(tokd_input[\"input_ids\"], skip_special_tokens=False)[0]\n",
    "\n",
    "    torch.manual_seed(gen_seed.value)\n",
    "\n",
    "    if not watermark:\n",
    "        output = generate_without_watermark(**tokd_input)\n",
    "    else:\n",
    "        output = generate_with_watermark(**tokd_input)\n",
    "\n",
    "\n",
    "    # need to isolate the newly generated tokens\n",
    "    output = output[:,tokd_input[\"input_ids\"].shape[-1]:]\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "\n",
    "    return (redecoded_input,\n",
    "            int(truncation_warning),\n",
    "            decoded_output,\n",
    "            output,\n",
    "            entropy_processor.entropies,\n",
    "            entropy_processor.topks\n",
    "            ) \n",
    "\n",
    "def color_decode_html(output, tokenizer, mask_map, sep=\"\"):\n",
    "    cursor, colored = 0, copy.copy(output)\n",
    "    green_color, red_color = \"green\", \"red\"\n",
    "    \n",
    "    for token_id, green in mask_map:\n",
    "        decoded_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
    "\n",
    "        token_start = colored.find(decoded_token, cursor)\n",
    "\n",
    "        if token_start > -1:\n",
    "            mod_token = f\"<mark style=\\\"background-color:{green_color if green else red_color}\\\">{decoded_token}</mark>\"\n",
    "            colored = colored[:token_start] + mod_token + colored[token_start + len(decoded_token):]\n",
    "            cursor = token_start + len(mod_token)\n",
    "    \n",
    "    return colored\n",
    "\n",
    "def color_decode_html_entropy(output, tokens, tokenizer, entropies, threshold = None, sep=\"\"):\n",
    "    cursor, colored = 0, copy.copy(output)\n",
    "\n",
    "    if not threshold:\n",
    "        scaled_ents = [0.1 + 0.9 * (ent / (max(entropies) - min(entropies)))**2 for ent in entropies]\n",
    "    else:\n",
    "        scaled_ents = [1. if ent >= threshold else 0. for ent in entropies]\n",
    "        \n",
    "    for idx, entropy in enumerate(scaled_ents):\n",
    "        decoded_token = tokenizer.decode(tokens[0][idx], skip_special_tokens=True)\n",
    "\n",
    "        token_start = colored.find(decoded_token, cursor)\n",
    "        \n",
    "        if token_start > -1:\n",
    "            mod_token = f\"<mark style=\\\"background-color:rgba(255,255,0,{entropy:.2f})\\\">{decoded_token}</mark>\"\n",
    "            colored = colored[:token_start] + mod_token + colored[token_start + len(decoded_token):]\n",
    "            cursor = token_start + len(mod_token)\n",
    "    \n",
    "    return colored\n",
    "    \n",
    "def detect(input_text, device=None, tokenizer=None, window_size=None, window_stride=None):\n",
    "    \"\"\"Instantiate the WatermarkDetection object and call detect on\n",
    "        the input text returning the scores and outcome of the test\"\"\"\n",
    "\n",
    "    watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=gamma.value, # should match original setting\n",
    "                                        seeding_scheme=seeding_scheme.value, # should match original setting\n",
    "                                        device=device, # must match the original rng device type\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=z_threshold.value,\n",
    "                                        normalizers='',\n",
    "                                        ignore_repeated_ngrams=True,\n",
    "                                        select_green_tokens=True)\n",
    "    score_dict = watermark_detector.detect(input_text, window_size=window_size, window_stride=window_stride)\n",
    "    # output = str_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    output = list_format_scores(score_dict, watermark_detector.z_threshold)\n",
    "    return output, watermark_detector.g_masks\n",
    "\n",
    "def format_names(s):\n",
    "    \"\"\"Format names for the gradio demo interface\"\"\"\n",
    "    s=s.replace(\"num_tokens_scored\",\"Tokens Counted (T)\")\n",
    "    s=s.replace(\"num_green_tokens\",\"# Tokens in Greenlist\")\n",
    "    s=s.replace(\"green_fraction\",\"Fraction of T in Greenlist\")\n",
    "    s=s.replace(\"z_score\",\"z-score\")\n",
    "    s=s.replace(\"p_value\",\"p value\")\n",
    "    s=s.replace(\"prediction\",\"Prediction\")\n",
    "    s=s.replace(\"confidence\",\"Confidence\")\n",
    "    return s\n",
    "    \n",
    "def list_format_scores(score_dict, detection_threshold):\n",
    "    \"\"\"Format the detection metrics into a gradio dataframe input format\"\"\"\n",
    "    lst_2d = []\n",
    "    # lst_2d.append([\"z-score threshold\", f\"{detection_threshold}\"])\n",
    "    for k,v in score_dict.items():\n",
    "        if k=='green_fraction': \n",
    "            lst_2d.append([format_names(k), f\"{v:.1%}\"])\n",
    "        elif k=='confidence': \n",
    "            lst_2d.append([format_names(k), f\"{v:.3%}\"])\n",
    "        elif isinstance(v, float): \n",
    "            lst_2d.append([format_names(k), f\"{v:.3g}\"])\n",
    "        elif isinstance(v, bool):\n",
    "            lst_2d.append([format_names(k), (\"Watermarked\" if v else \"Human/Unwatermarked\")])\n",
    "        else: \n",
    "            lst_2d.append([format_names(k), f\"{v}\"])\n",
    "    if \"confidence\" in score_dict:\n",
    "        lst_2d.insert(-2,[\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    else:\n",
    "        lst_2d.insert(-1,[\"z-score Threshold\", f\"{detection_threshold}\"])\n",
    "    return lst_2d\n",
    "\n",
    "def change_to_dict_w_keys(ll, suffix):\n",
    "    \"\"\"\"\"\"\n",
    "    return {\n",
    "        f\"{k[0]} ({suffix})\":k[1] for k in ll\n",
    "    }\n",
    "\n",
    "def display_process_msg(msg):\n",
    "    display(HTML(f\"{CSS_BASE}\\n<hr><div style=\\\"text-align: center\\\">{SPINNER}</div><b><div style=\\\"text-align: center\\\">{msg}</b></div>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb2f64-769d-43ea-b311-8839ef5db079",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba8ea66-d7e3-4ba1-87a3-41c67a4d218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extended_watermark_processor import WatermarkLogitsProcessor, WatermarkDetector, WatermarkBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a432e467-1423-4b4c-b289-f5cfe268c0e5",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c77281e10cc4ae482ce0d56ca6151b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Who is Robert Downey Jr?', description='Prompt:', layout=Layout(height='150px', margin='10px 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9823e3874d7c40828ea4d64322c93b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Watermark', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7afe8877c61344b3b68a7a2e3925610d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Generate', layout=Layout(margin='10px 0px 10px 0px', width='200px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f5e413e1a4d7a89e65c8fad885123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = widgets.Textarea(\n",
    "    value='Who is Robert Downey Jr?',\n",
    "    placeholder='Prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='70%', height='150px', margin='10px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "watermark = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Watermark',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "# Create a Button widget with layout properties\n",
    "button = widgets.Button(\n",
    "    description='Generate',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px', margin='10px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "CSS_BASE = \"<style>\" + CSS_BASE + \"</style>\"\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display_process_msg(f\"Generating using {model_name.value} {'(Watermarked)' if watermark.value else ''}\")\n",
    "        inp, trunc, out, tokens, entropies, topks = generate(prompt.value, model, device,tokenizer, watermark.value)\n",
    "        entropies = [ent.item() for ent in entropies]\n",
    "        \n",
    "        topks = [\n",
    "            [entropies[idx], tokenizer.decode(tokens[0][idx], skip_special_tokens=True), [\n",
    "                (ind, val, tokenizer.decode(ind, skip_special_tokens=True)) for ind, val in token\n",
    "            ]] for idx, token in enumerate(topks)\n",
    "        ]\n",
    "        \n",
    "        open(\"data.json\", \"w\").write(\"const RAW_DATA = \" + json.dumps({\"text\": out, \"probs\":topks}) + \";\")\n",
    "        \n",
    "        output.clear_output()\n",
    "        display_process_msg(\"Running Detection\")\n",
    "        result_wm, g_masks = detect(out, device, tokenizer)\n",
    "        # generate\n",
    "        \n",
    "        color_coded = color_decode_html(out, tokenizer, g_masks)\n",
    "        entropy_coded = color_decode_html_entropy(out, tokens, tokenizer, entropies)\n",
    "        entropy_coded_th = color_decode_html_entropy(out, tokens, tokenizer, entropies, threshold=0.5)\n",
    "        \n",
    "        # sweet detector\n",
    "        watermark_detector = SweetDetector(\n",
    "                                        device=device,\n",
    "                                        vocab=list(tokenizer.get_vocab().values()),\n",
    "                                        gamma=gamma.value,\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        z_threshold=z_threshold.value,\n",
    "                                        ignore_repeated_bigrams=False,\n",
    "                                        entropy_threshold=0.5)\n",
    "\n",
    "        tokd_input = tokenizer(inp, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(device)\n",
    "        \n",
    "        detection_result = watermark_detector.detect( # no batch\n",
    "                        tokenized_prefix=tokd_input[0],\n",
    "                        tokenized_text=tokens[0],\n",
    "                        entropy=entropies,\n",
    "                    )\n",
    "\n",
    "        output.clear_output()\n",
    "        display(HTML(f\"<pre>{color_coded}</pre><hr><pre>{out}</pre><hr>{result_wm}<hr><pre>{entropy_coded}</pre><hr><pre>{entropy_coded_th}</pre><hr>{detection_result}\"))\n",
    "        \n",
    "button.on_click(on_button_click)\n",
    "\n",
    "display(prompt, watermark, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1fbe04-cb85-4bf6-b4f0-02846dc965c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Text here...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='70%', height='150px', margin='10px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "output2 = widgets.Output()\n",
    "\n",
    "button2 = widgets.Button(\n",
    "    description='Detect',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='200px', margin='10px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "def on_button_click2(b):\n",
    "    with output2:\n",
    "        output2.clear_output()\n",
    "        display_process_msg(\"Running Detection\")\n",
    "        result_wm, g_masks = detect(test_text.value, device, tokenizer)\n",
    "        # generate\n",
    "        output2.clear_output()\n",
    "        color_coded = color_decode_html(test_text.value, tokenizer, g_masks)\n",
    "\n",
    "        display(HTML(f\"<pre>{color_coded}</pre><hr>{result_wm}\"))\n",
    "\n",
    "button2.on_click(on_button_click2)\n",
    "\n",
    "display(test_text, button2, output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
